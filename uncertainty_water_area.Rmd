---
title: "What is the relation between irrigated areas and irrigation water withdrawal?"
author: "Arnald Puy et al."
header-includes:
  - \usepackage[font=footnotesize]{caption}
  - \usepackage{dirtytalk}
  - \usepackage{booktabs}
  - \usepackage{tabulary}
  - \usepackage{enumitem}
  - \usepackage{lmodern}
  - \usepackage[T1]{fontenc}
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
    keep_tex: true
  word_document:
    toc: no
    toc_depth: '2'
link-citations: yes
fontsize: 11pt
bibliography: 
  - /Users/arnald/Documents/bibtex/R_packages.bib
  - /Users/arnald/Documents/bibtex/LATEX_water_withdrawal.bib
  - /Users/arnald/Documents/bibtex/LATEX_population.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage 

# Introduction

Current figures suggest that irrigated agriculture consumes c. 70\% of all freshwater resources. The total amount of water used by irrigation agriculture is determined by 
factors such as soil hydraulic parameters, crop types, the weather or the irrigated area [@Wisser2008], with the latter being especially influential [@Puy2017]. It is thus relevant to empirically describe the relationship between the irrigated area and the volume of water required for irrigation: what happens when we increase the total extension of irrigation?  does the demand for irrigation water increase proportionally, or in a non-linear fashion? Are small irrigated areas more water efficient, or do they disproportionally require more water than large ones?

# Materials and methods

Here we aim at tackling these questions. Let us first create a wrapper function that allows to load all the required `R` libraries in one go. We then load the package `checkpoint` [@MicrosoftCorporation2018], which installs in a local directory the same package versions used in the study. This allows anyone that runs our code to fully reproduce our results anytime. Finally, we cast a function to define the theme of the figures we will plot to present our results.

```{r load_packages, results="hide", message=FALSE, warning=FALSE}

# LOAD PACKAGES ---------------------------------------------------------------

# Function to read in all required packages in one go:
loadPackages <- function(x) {
  for(i in x) {
    if(!require(i, character.only = TRUE)) {
      install.packages(i, dependencies = TRUE)
      library(i, character.only = TRUE)
    }
  }
}

loadPackages(c("data.table", "ggplot2", "sensobol", "scales",
               "ncdf4", "rworldmap", "sp", "countrycode", 
               "dplyr", "IDPmisc", "boot", "parallel", 
               "MASS", "doParallel", "complmrob", 
               "mvoutlier", "sandwich", "lmtest", "mice", 
               "ggridges", "broom", "naniar", "cowplot", 
               "tidyr", "benchmarkme"))

# SET CHECKPOINT --------------------------------------------------------------

dir.create(".checkpoint")

library("checkpoint")

checkpoint("2019-09-10", 
           R.version ="3.6.1", 
           checkpointLocation = getwd())

# CUSTOM FUNCTION TO DEFINE THE PLOT THEMES -----------------------------------

theme_AP <- function() {
  theme_bw() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.background = element_rect(fill = "transparent",
                                           color = NA),
          legend.key = element_rect(fill = "transparent",
                                    color = NA))
}
```

Since we will need to read in several global datasets and clean the data before conducting any test, we define here several functions to that aim. Firstly, the function `country_code` will ensure that all countries receive the same name across datasets. Secondly, the `coords2country` function will translate coordinates into the country name. Finally, the `get_nc` function will allow to read in `.nc` files, a format widely used to store spatially explicit data on irrigation water withdrawal.

```{r functions_data, cache=TRUE}

# CREATE FUNCTIONS ------------------------------------------------------------

# Function to obtain UN code, Continent and Country names
country_code <- function(dt) {
  dt[, `:=` (Code = countrycode(dt[, Country], 
                                origin = "country.name", 
                                destination = "un"), 
             Continent = countrycode(dt[, Country], 
                                     origin = "country.name", 
                                     destination = "continent"))]
  dt[, Country:= countrycode(dt[, Code], 
                             origin = "un", 
                             destination = "country.name")]
  setcolorder(dt, c("Country", "Continent", "Code", "Water.Withdrawn"))
  return(dt)
}

## Function to transform longitude and latitude to country.
# It is borrowed from Andy:
# https://stackoverflow.com/questions/14334970/convert-latitude-and-longitude-coordinates-to-country-name-in-r)
coords2country = function(points) {  
  countriesSP <- rworldmap::getMap(resolution = 'low')
  pointsSP = sp::SpatialPoints(points, proj4string=CRS(proj4string(countriesSP)))  
  indices = sp::over(pointsSP, countriesSP)
  indices$ADMIN  
  #indices$ISO3 # returns the ISO3 code 
  #indices$continent   # returns the continent (6 continent model)
  #indices$REGION   # returns the continent (7 continent model)
}

# Function to load and extract data from .nc files
get_nc_data <- function(nc_file) {
  nc <- nc_open(nc_file)
  ww <- ncvar_get(nc, "withd_irr")
  lon <- ncvar_get(nc, "lon")
  lat <- ncvar_get(nc, "lat")
  water <- rowSums(ww[, 469:ncol(ww)]) # Obtain year values for 2010 only
  ww.df <- data.frame(cbind(lon, lat, water)) 
  countries <- coords2country(ww.df[1:nrow(ww.df), 1:2])
  df <- cbind(countries, ww.df)
  setDT(df)
  final <- df[, .(Water.Withdrawn = sum(water)), countries]
  setnames(final, "countries", "Country")
  country_code(final)
  out <- na.omit(final[order(Continent)])
  out[, Water.Withdrawn:= Water.Withdrawn / 1000] # From mm to m
  return(out)
}
```

## Irrigation water withdrawal datasets

There are several datasets and Global Hydrological Models (GHM) providing information on irrigation water withdrawal at the country level, with significant differences on the values reported. Here we consider this source of uncertainty through the following datasets:
\begin{enumerate}
\item The WaterGap GHM [@Doll2002].
\item The LPJmL GHM.
\item The H08 GHM.
\item The PCR-GLOBWB GHM.
\item FAOSTAT irrigation water withdrawal (Table 4).
\item @Liu2016a dataset (Aquastat dataset with all the missing values filled).
\end{enumerate}
In the next code chunk we read in all these datasets, clean them and merge them to obtain a final dataset with all the data on irrigation water withdrawal merged.

```{r water_with_dataset, cache=TRUE, warning=FALSE}

# READ IN DATASETS ON IRRIGATION WATER WITHDRAWAL -----------------------------

# FAO data (Table 4) ----------------------------
# http://www.fao.org/nr/water/aquastat/water_use_agr/IrrigationWaterUse.pdf

# UNIT IS KM3/YEAR
table4.tmp <- fread("table_4.csv", skip = 3, nrows = 167) %>%
  .[, .(Country, Year, Water.withdrawal)] %>%
  setnames(., "Water.withdrawal", "Water.Withdrawn")

# Extract the selected years
table4.dt <- country_code(table4.tmp[Year %in% 1999:2012])[
  , Water.Dataset:= "Table 4"][
    , Year:= NULL]

# Liu et al. dataset ----------------------------
#UNIT IS 10^9 m3/year = km3/year
liu.dt <- fread("liu.csv")[, .(country, irr)] %>%
  setnames(., c("country", "irr"), c("Country", "Water.Withdrawn")) %>%
  country_code(.) %>%
  .[, Water.Dataset:= "Liu et al. 2016"]

# Huang et al datasets --------------------------
names_nc_files <- c("withd_irr_lpjml.nc", "withd_irr_pcrglobwb.nc", 
                    "withd_irr_h08.nc", "withd_irr_watergap.nc")
out.nc <- lapply(names_nc_files, function(x) get_nc_data(x))
names(out.nc) <- c("LPJmL", "PCR-GLOBWB", "H08", "WaterGap")

GHM.dt <- rbindlist(out.nc, idcol = "Water.Dataset") %>%
  .[order(Country)]
```

Not all irrigation water withdrawal datasets provide measurements for the same countries. This means that any computation conducted using a given dataset risks being biased by the specific countries included in it. In order to tackle this source of uncertainty, all datasets should include the same countries, regardless of whether there is a measurement available for the country in question. Later on the uncertainties related with the imputation of missing values will be dealt with. For now, we extract a vector with the name of all the different countries mentioned by any of the irrigated water withdrawal datasets, and merge it with each single water withdrawal dataset - missing values will be for the moment treated as `NA`.

```{r arrange_total_countries, cache=TRUE, dependson="water_with_dataset"}

# ARRANGE THE TOTAL NUMBER OF COUNTRIES ---------------------------------------

# Read in list of countries in UN
DT <- fread("UN_countries2.csv", select = "Country")

# Give standard country names, UN codes and link with Continent
DT <- DT[, `:=` (Code = countrycode(DT[, Country], 
                                    origin = "country.name", 
                                    destination = "un"), 
                 Continent = countrycode(DT[, Country], 
                                         origin = "country.name", 
                                         destination ="continent"))]

# Manually add Micronesia
DT <- DT[, Continent:= ifelse(Country %in% "Micronesia", "Oceania", Continent)]
```

```{r final_water_dataset, cache=TRUE, dependson=c("water_with_dataset" ,"arrange_total_countries")}

# CREATE THE FINAL IRRIGATION WATER WITHDRAWAL DATASET ------------------------

# Merge with the Country vector
tmp <- GHM.dt[, merge(.SD, DT, by = c("Country", "Code", "Continent"), 
                      all.y = TRUE), by = Water.Dataset]

# Check whether there are duplicated Countries
tmp[tmp[, duplicated(Country), Water.Dataset][, V1]]

# Get mean values for the duplicated Countries
GHM.dt.full <- tmp[, .(Water.Withdrawn = mean(Water.Withdrawn)), 
                   .(Water.Dataset, Country, Code, Continent)]

# Arrange Liu data set
liu.dt.full <- merge(DT, liu.dt, 
                     by = c("Country", "Code", "Continent"), 
                     all.x = TRUE) %>%
  .[, Water.Dataset:= ifelse(is.na(Water.Dataset), 
                             "Liu et al. 2016", 
                             Water.Dataset)]

# Arrange Table 4 dataset
table4.dt.full <- merge(DT, table4.dt, 
                        by = c("Country", "Code", "Continent"), 
                        all.x = TRUE) %>%
  .[, Water.Dataset:= ifelse(is.na(Water.Dataset), 
                             "Table 4", 
                             Water.Dataset)]

# Obtain final irrigation water withdrawal dataset
water.dt <- rbind(liu.dt.full, table4.dt.full, GHM.dt.full)
```

## Irrigated area datasets

There are also several datasets informing on the extension of irrigation at the country level, with large uncertainties (sometimes the values differ by more than one order of magnitude). Here we use the data compiled by @Meier2018, which also includes the datasets by Aquastat [@FAO2016b], FAOSTAT [@FAO2017a],  @Siebert2013, @Salmon2015 and @Thenkabail2009.

```{r area_dataset, cache=TRUE}

# READ IN IRRIGATED AREA DATASETS ---------------------------------------------

meier.dt <- fread("meier.csv") %>%
  setnames(., "Codes", "Code")
```

Finally, we bind the datasets on irrigation water withdrawal and irrigated area, and obtain the full dataset we will use in our analysis. We also plot the data, which evidences that irrigation water withdrawal and irrigated area are indeed related (Fig. 1). The last code chunk log-transforms these parameters to ease the interpretation of the results and prepare the dataset for the upcoming analysis.

```{r merge_with_area, cache=TRUE, dependson=c("area_dataset", "water_with_dataset", "final_water_dataset")}

# MERGE DATASETS --------------------------------------------------------------

irrigated.area.datasets <- colnames(meier.dt)[-c(1:3)]

irrigated.dt <- melt(meier.dt, measure.vars = irrigated.area.datasets) %>%
  .[, merge(.SD, DT, by = c("Country", "Code", "Continent"), 
            all.y = TRUE), by = variable] %>%
  setnames(., c("variable", "value"), c("Area.Dataset", "Irrigated.Area"))

tmp.dt <- merge(irrigated.dt, water.dt, on = c("Continent", "Country", "Code"), 
                allow.cartesian = TRUE) %>%
  .[!Continent == "Oceania"] # Drop Oceania

# Vector with the countries to drop
countries.drop <- tmp.dt[Water.Withdrawn == 0 & is.na(Irrigated.Area) == TRUE] %>%
  .[, unique(Country)]

# Drop the countries
full.dt <- tmp.dt[!Country %in% countries.drop]
```

```{r export.irrigated.dt, cache=TRUE, dependson="merge_with_area"}

# EXPORT IRRIGATED AREA DT ----------------------------------------------------

fwrite(irrigated.dt, "irrigated.dt.csv")
```

```{r plot_merged, cache=TRUE, dependson="merge_with_area", dev="tikz", fig.height=8, fig.width=7, fig.cap="Scatter plots of irrigated areas (x-axis) against irrigation water withdrawal (y-axis). All the possible combinations of datasets are shown."}

# PLOT ------------------------------------------------------------------------

full.dt %>%
  ggplot(., aes(Irrigated.Area, Water.Withdrawn, 
                color = Continent)) +
  geom_point(size = 0.8) +
  scale_x_log10(breaks = trans_breaks("log10", function(x) 10 ^ (2 * x)),
                labels = trans_format("log10", math_format(10 ^ .x))) +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10 ^ x),
                labels = trans_format("log10", math_format(10 ^ .x))) +
  labs(x = "Irrigated area (Mha)", 
       y = expression(paste("Water withdrawal", "", 10^9, m^3/year))) +
  facet_grid(Water.Dataset ~ Area.Dataset) +
  theme_AP() +
  theme(legend.position = "top", 
        strip.text = element_text(size = 7))
```

```{r log10, cache=TRUE, dependson="merge_with_area"}

# TRANSFORM DATASET -----------------------------------------------------------

cols <- c("Water.Withdrawn", "Irrigated.Area")
col_names <- c("Continent", "Water.Dataset", "Area.Dataset", "Regression", 
               "Imputation.Method", "Iteration")
cols_group <- c("Continent", "Area.Dataset", "Water.Dataset")
full.dt <- full.dt[, (cols):= lapply(.SD, log10), .SDcols = (cols)]
```

```{r export_dataset_log10, cache=TRUE, dependson="log10"}

# EXPORT FULL DATASET WITH MISSING VALUES --------------------------------------------

fwrite(full.dt, "full.dt.csv")
```

```{r plot_missing, cache=TRUE, dependson="log10", dev="tikz", fig.height=8, fig.width=7}

# SCATTERPLOT SHOWING MISSING VALUES ------------------------------------------

scatter.na <- copy(full.dt)
cols_transform <- c("Irrigated.Area", "Water.Withdrawn")
scatter.na[, (cols_transform):= lapply(.SD, function(x) 10 ^ x), .SDcols = cols_transform]
tmp <- split(scatter.na, scatter.na$Continent)

gg <- list()
for(i in names(tmp)) {
  gg[[i]] <- ggplot(tmp[[i]], aes(Irrigated.Area, Water.Withdrawn)) +
    geom_miss_point() +
    facet_grid(Water.Dataset ~ Area.Dataset) + 
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10 ^ (2 * x)),
                labels = trans_format("log10", math_format(10 ^ .x))) +
    scale_y_log10(breaks = trans_breaks("log10", function(x) 10 ^ (2 * x)),
                  labels = trans_format("log10", math_format(10 ^ .x))) +
    labs(x = "Irrigated area (Mha)", 
         y = expression(paste("Water withdrawal", "", 10^9, m^3/year))) +
    theme_AP() +
    theme(legend.position = "top",
          strip.text = element_text(size = 7)) +
    ggtitle(names(tmp[i]))
}

gg
```

```{r plot_missing2, cache=TRUE, dependson="log10", fig.height=8, fig.width=4}

# PLOT PERCENTAGE OF MISSING --------------------------------------------------

na.dt <- full.dt[, .(Continent, Country, Irrigated.Area, Water.Withdrawn)]
tmp <- split(na.dt[, !"Continent", with = FALSE], na.dt$Continent)

gg <- list()
for(i in names(tmp)) {
  gg[[i]] <- gg_miss_fct(x = tmp[[i]], fct = Country) +
    coord_flip() +
    labs(x = "Parameter", 
         y = "") +
    viridis::scale_fill_viridis(name = "Missing") +
    theme(legend.position = "top") +
    ggtitle(names(tmp[i]))
}

gg
```



## Assesment of uncertainties in the model structure

As shown in Fig. 1, there is a clear relation between irrigated areas and irrigation water withdrawal, more or less sharp depending on which combination of datasets are used. This relationship is prone to be described by OLS regressions, and the resulting slope be used to ascertain whether irrigation water withdrawal tends to scale superlinearly ($\beta > 1$) or sublinearly ($\beta < 1$) for every increase in the area under irrigation. However, besides the uncertainty derived from the different measurements of irrigated area and irrigation water withdrawal available, there is uncertainty with regards to the selected model structure for OLS: robust/non-robust approaches to correct for outliers, whether a correction for heteroskedasticity is needed, which methodology is used to imput missing values, and uncertainty with regards to the true value of $\beta$.

Hereafter we explore the first three sources of uncertainty just mentioned. 

### Multivariate outliers

We first check the existence of bivariate outliers using Mahalanobis classic and robust distances [@Filzmoser2005]. The results, which are presented in Fig. 2, confirm the presence of extreme values and justify the use of a trigger to decide whether to use robust or non-robust regression methods.

```{r outliers, cache=TRUE, fig.keep="none", echo=FALSE,results='hide', dependson="log10"}

# CHECK OUTLIERS -------------------------------------------------------------

tmp <- NaRV.omit(full.dt)[, dd.plot(.SD), .SDcols = cols, cols_group]
```

```{r plot_outliers, cache=TRUE, dependson="outliers", dev="tikz", fig.height=8.5, fig.width=6.5, fig.cap="Bivariate outliers."}

# PLOT OUTLIERS ---------------------------------------------------------------

ggplot(tmp, aes(md.cla, md.rob, 
                shape = outliers, 
                color = Continent)) +
  geom_point() +
  scale_shape_manual(name = "Outlier", 
                     labels = c("No", "Yes"), 
                     values = c(16, 4)) +
  facet_grid(Water.Dataset ~ Area.Dataset) +
  labs(x = "Mahalanobis distance", 
       y = "Robust distance") +
  theme_AP() +
  theme(legend.position = "top", 
        strip.text = element_text(size = 7))
```

### Heteroskedasticity

In order to see whether the data exhibits heteroskedasticity, we compute OLS regressions for each combination of irrigated area and water withdrawal dataset, and plot the residual against the fitted values. The results are plotted in Figs 3-6. As can be seen, the variance is not constant for some specific combinations of datasets. This justifies including the correction for heteroskedasticity as a source of structural uncertainty in the computation of the regression equations.

```{r hetero, cache=TRUE, dependson="merge_with_area"}

# CHECK HETEROSKEDASTICITY ----------------------------------------------------

hetero <- NaRV.omit(full.dt)[, .(model = .(lm(Water.Withdrawn ~ Irrigated.Area))), cols_group]
```

```{r hetero_plot, cache=TRUE, dependson="hetero", dev="tikz", fig.height=8.5, fig.width=6.5, fig.cap="Residual versus fitted values."}

# PLOT HETEROSKEDASTICITY ----------------------------------------------------

tmp <- hetero[, c("resid" = lapply(model, residuals), 
                  "pred" = lapply(model, fitted)), cols_group]

hetero.plot <- split(tmp, tmp$Continent)

gg <- list()
for(i in names(hetero.plot)) {
  gg[[i]] <- ggplot(hetero.plot[[i]], aes(pred, resid)) +
    geom_point() +
    facet_grid(Area.Dataset ~ Water.Dataset) +
    theme_AP() +
    labs(x = "Fitted", 
         y = "Residual") +
    geom_hline(yintercept = 0, 
               lty = 2) +
    ggtitle(names(hetero.plot[i]))
}

gg
```

### Missing values

There are several methods to impute missing values, each with its specificities. The selection of the imputation method might therefore influence the results of the model. In this study we will take into account this source of structural uncertainty by analysing the effects of three different imputation methods: linear regression (`norm.predict`), stochastic regression (`norm.nob`) and random forest (`rf`). Since these imputation models are based on random sampling, we should also study the influence of randomness in the obtention of the final imputed missing value. Here we check that source of uncertainty by limiting the random sampling to 5 different imputed data sets for each imputation method.

```{r missing_values, cache=TRUE, dependson="log10", echo=FALSE, message=FALSE, results='hide', warning=FALSE}

# IMPUTATION OF MISSING VALUES -------------------------------------------------

# Substitute Inf values for NA
for (j in 1:ncol(full.dt)) set(full.dt, which(is.infinite(full.dt[[j]])), j, NA)

full.dt[, lapply(.SD, function(x) sum(is.infinite(x)))] # Check

# Imputation settings
m.iterations <- 20
imputation.methods <- c("rf", "norm.boot", "norm.nob")
set.seed(10) # Set seed to allow for replication

# Run
imput <- full.dt[, .(Group = lapply(imputation.methods, function(x) 
  mice(.SD, m = m.iterations, maxit = m.iterations, method = x, seed = 500, 
       print = FALSE))), 
  cols_group]

imput <- imput[, Imputation.Method:= rep(imputation.methods, .N / length(imputation.methods))]

# Extract iterations
imput <- imput[, Datasets:= lapply(Group, function(x) 
  lapply(1:m.iterations, function(y) data.table(mice::complete(x, y))))] %>%
  .[, Data:= lapply(Datasets, function(x) rbindlist(x, idcol = "Iteration"))]

# Vector to loop onto
columns_add <- c("Country", "Iteration", "Irrigated.Area", "Water.Withdrawn")
tmp <- as.list(columns_add)
names(tmp) <- columns_add

# Extract columns
for(i in names(tmp)) {
  imput <- imput[, tmp[[i]]:= lapply(.SD, function(x) 
    lapply(x, function(y) y[, ..i])), .SDcols = "Data"]
}

# Unlist
full.imput <- imput[, lapply(.SD, unlist), 
                    .SDcols = columns_add, 
                    .(Continent, Area.Dataset, Water.Dataset, Imputation.Method)]
```

# Linear regressions and residuals

```{r conduct_lm, cache=TRUE, dependson="missing_values"}

# LINEAR REGRESSIONS AND PULL RESIDUALS ---------------------------------------

# Set grouping vectors
cols_group <- c("Continent", "Area.Dataset", "Water.Dataset")
all.cols <- c(cols_group, "Imputation.Method", "Iteration")

dt <- full.imput[, .(fit = list(list(lm(Water.Withdrawn ~ Irrigated.Area)))), all.cols] %>%
  .[, lapply(fit, function(x) lapply(x, function(y) augment(y))), all.cols]

resid.dt <- dt[, lapply(V1, function(x) unlist(x$.resid)), all.cols] %>%
  .[, Country:= full.imput$Country] %>%
  .[, Method:= "Normal"]

dtR <- full.imput[, .(fit = list(list(rlm(Water.Withdrawn ~ Irrigated.Area, 
                                          maxit = 60)))), all.cols] %>%
  .[, lapply(fit, function(x) lapply(x, function(y) augment(y))), all.cols]


resid.dtR <- dtR[, lapply(V1, function(x) unlist(x$.resid)), all.cols] %>%
  .[, Country:= full.imput$Country] %>%
  .[, Method:= "Robust"]

final.resid <- rbind(resid.dt, resid.dtR)
```

```{r plot_residuals, cache=TRUE, dependson="conduct_lm", fig.height=8, fig.width=6, warning=FALSE}

 # PLOT RESIDUALS -------------------------------------------------------------

# function to plot
gg_bar <- gg_ridge <- list()
for(i in c("Africa", "Americas", "Asia", "Europe")) {
  country.vector <- final.resid[Continent == i] %>%
    .[order(V1)] %>%
    .$Country %>%
    unique(.)
  gg_ridge[[i]] <- final.resid[Continent == i] %>%
    .[, Country:= factor(Country, levels = country.vector)] %>%
    ggplot(., aes(V1, Country, fill = ..x..)) +
    geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
    geom_vline(xintercept = 0, 
               lty = 2) +
    scale_fill_gradient2(name = "Residual", 
                         low = "blue", mid = "white",
                         high = "red", midpoint = 0) +
    labs(x = "Residual", 
         y = "") +
    theme_AP() + 
    theme(legend.position = "top")
  gg_bar[[i]] <- final.resid[Continent == i] %>%
    .[, sum(V1 > 0) / .N, Country] %>%
    ggplot(., aes(reorder(Country, V1), V1, 
                  fill = ..y..)) + 
    geom_bar(stat = "identity") +
    coord_flip() + 
    labs(y = "Residuals > 0 (%)", 
         x = "") +
    theme_AP() + 
    theme(legend.position = "top")
}

gg_ridge
```

```{r export_dataset_imputation, cache=TRUE, dependson="missing_values"}

# EXPORT FULL DATASET WITHOUT MISSING VALUES --------------------------------------------

fwrite(full.imput, "full.imput.csv")

```

# Compute all possible beta values

```{r compute_beta, cache=TRUE, dependson="missing_values"}

# COMPUTE BETA AND R^2 --------------------------------------------------------

regressions <- full.imput[, .(Normal = coef(lm(Water.Withdrawn ~ Irrigated.Area)), 
                              Robust = coef(rlm(Water.Withdrawn ~ Irrigated.Area, maxit = 60)), 
                              r.squared = summary(lm(Water.Withdrawn ~ Irrigated.Area))$r.squared), 
                      all.cols] 

results <- regressions[, Type:= rep(c("Intercept", "Beta"), times = nrow(.SD) / 2)] %>%
  melt(., measure.vars = c("Normal", "Robust"), variable.name = "Regression") %>%
  tidyr::spread(., Type, value) %>%
  .[, index:= paste(Continent, Area.Dataset, Water.Dataset, Regression, 
                   Imputation.Method, Iteration, sep = "_")]

```

```{r export_beta_results, cache=TRUE, dependson="compute_beta"}

# EXPORT BETA AND R^2 RESULTS -------------------------------------------------

results <- regressions[, Type:= rep(c("Intercept", "Beta"), times = nrow(.SD) / 2)] %>%
  melt(., measure.vars = c("Normal", "Robust"), variable.name = "Regression") %>%
  spread(., Type, value) %>%
  .[, index:= paste(Continent, Area.Dataset, Water.Dataset, Regression, 
                   Imputation.Method, Iteration, sep = "_")]

fwrite(results, "results.csv")
```

## Create lookup table

Here we define the lookup table that we will use to select the $\beta$ value according to the conditions set by the triggers in the sample matrix (see below). 

```{r lookup, cache=TRUE, dependson="compute_beta"}

# CREATE LOOKUP TABLE ---------------------------------------------------------

lookup <- setkey(results, index)

# Export lookup
fwrite(lookup, "lookup.csv")
```

# The sample matrix

This section designs the sample matrix that will be used to compute the model output, i.e. the $\beta$ value defined by the conditions set by the triggers. Firstly, we pool all irrigated area and water withdrawal datasets and calculate the minimum and maximum values for each of these parameters. This is needed to bound the uniform distributions with which we will characterize $X_1$ (i.e. the trigger to select which irrigated area data set to use) and $X_2$ (i.e. the trigger to select which irrigation water withdrawal data set to use). We also define the initial sample size of the sample matrix ($N = 2^{13}$), for which we use Sobol’ Quasi Random Numbers, and create a function to transform its columns to their appropriate distributions.

```{r set_sample_matrix, cache=TRUE}

# DEFINE THE SETTINGS OF THE SAMPLE MATRIX ------------------------------------

Continents <- c("Africa", "Americas", "Asia", "Europe")

# Create a vector with the name of the columns
parameters <- paste("X", 1:5, sep = "")

# Select sample size
n <- 2 ^ 13

# Define order
order <- "third"
```

```{r sample_matrix, cache=TRUE, dependson="set_sample_matrix"}

# CREATE THE SAMPLE MATRIX ----------------------------------------------------

# Create an A, B and AB matrices for each continent
sample.matrix <- lapply(Continents, function(Continents) 
  sobol_matrices(N = n,
                 params = parameters,
                 order = order) %>%
    data.table())

# Name the slots, each is a continent
names(sample.matrix) <- Continents

# Name the columns
sample.matrix <- lapply(sample.matrix, setnames, parameters)
```

```{r transform_sample_matrix, cache=TRUE, dependson=c("sample_matrix", "set_sample_matrix")}

# TRANSFORM THE SAMPLE MATRIX -------------------------------------------------

# Transform the sample matrix
transform_sample_matrix <- function(dt) {
  dt[, X1:= floor(X1 * (6 - 1 + 1)) + 1] %>%
    .[, X1:= ifelse(X1 == 1, "Aquastat", 
                    ifelse(X1 == 2, "FAOSTAT", 
                           ifelse(X1 == 3, "Siebert.et.al.2013", 
                                  ifelse(X1 == 4, "Meier.et.al.2018", 
                                         ifelse(X1 == 5, "Salmon.et.al.2015", 
                                                "Thenkabail.et.al.2009")))))] %>%
    .[, X2:= floor(X2 * (6 - 1 + 1)) + 1] %>%
    .[, X2:= ifelse(X2 == 1, "LPJmL", 
                    ifelse(X2 == 2, "H08", 
                           ifelse(X2 == 3, "PCR-GLOBWB", 
                                  ifelse(X2 == 4, "WaterGap", 
                                         ifelse(X2 == 5, "Table 4", 
                                                "Liu et al. 2016")))))] %>%
    .[, X3:= floor(X3 * (2 - 1 + 1)) + 1] %>%
    .[, X3:= ifelse(X3==1, "Normal", "Robust")] %>%
    .[, X4:= floor(X4 * (length(imputation.methods) - 1 + 1)) + 1] %>%
    .[, X4:= ifelse(X4 == 1, imputation.methods[1], 
                    ifelse(X4 == 2, imputation.methods[2], imputation.methods[3]))] %>%
    .[, X5:= floor(X5 * (m.iterations - 1 + 1)) + 1] 
}

sample.matrix <- lapply(sample.matrix, transform_sample_matrix)
sample.matrix.dt <- rbindlist(sample.matrix, idcol = "Continent")

fwrite(sample.matrix.dt, "sample.matrix.dt.csv")
```

We also print the sample matrix to better inform the reader about our analysis:
\begin{itemize}
\item X1: Trigger that decides which irrigated area data set to use.
\item X2: Trigger that decides which irrigation water withdrawal data set to use.
\item X3: Trigger that decides whether to select robust, non-robust or heteroskedasticity-corrected methods.
\item X4: Trigger that decides which imputation method should be used for missing values.
\item X5: Trigger that decides which iteration of a given imputation method to use.
\item X6: Trigger that decides which $\beta$ value to extract from the pool of bootstrap replicas, with 1 and 500 being respectively the $\beta$ value with the hlowest and highest values.
\end{itemize}

```{r print_matrix)}

# PRINT SAMPLE MATRIX ---------------------------------------------------------

print(sample.matrix.dt)
```

# The model

The model is simply a one-line function that runs in the sample matrix as follows: at the $v$-th row, it creates a vector with all the conditions set by $X_1^{(v)}, ..., X_6^{(v)}$, and uses this vector to extract from the lookup table the $\beta^{(v)}$ value according to these conditions.

```{r define_model, cache=TRUE}

# THE MODEL -------------------------------------------------------------------

model <- function(X) lookup[.(paste0(X[, 1:6], collapse = "_"))][, c(Intercept, Beta, r.squared)]
```

```{r run_model, cache=TRUE, dependson=c("define_model", "set_boot", "lookup", "transform_sample_matrix")}

# RUN THE MODEL---------------------------------------------------------------

# Set number of cores at 75%
n_cores <- floor(detectCores() * 0.75)

# Create cluster
cl <- makeCluster(n_cores)
registerDoParallel(cl)

# Run model in parallel
Y <- foreach(i=1:nrow(sample.matrix.dt),
             .packages = "data.table") %dopar%
  {
    model(sample.matrix.dt[i])
  }

# Stop parallel cluster
stopCluster(cl)
```

```{r arrange_output, cache=TRUE, dependson="run_model"}

# ARRANGE MODEL OUTPUT -------------------------------------------------------

sample.matrix.dt <- cbind(sample.matrix.dt, data.table(do.call(rbind, Y))) %>%
  setnames(., c("V1", "V2", "V3"), c("Intercept", "Beta", "r.squared"))

# Select the A and B matrix only (for uncertainty analysis)
AB.dt <- sample.matrix.dt[, .SD[1:(n * 2)], Continent]

# Export results
fwrite(sample.matrix.dt, "sample.matrix.dt.csv")
fwrite(AB.dt, "AB.dt.csv")
```

# Uncertainty analysis

Here we plot the distribution of $\beta$ for each continent, and calculate the proportion of model runs with $\beta<1$ (sublinear regime; larger irrigated areas are more water efficient) and $\beta>1$ (superlinear regime, larger irrigated areas are less water efficient). The results suggest that there are almost 9/10 chances and 3/4 chances of larger irrigated areas in Asia and Europe being more water efficient than smaller ones. In the case of the Americas and Africa, the chances are 3.5/5 and 1/2 respectively.

```{r plot_uncertainty, cache=TRUE, dependson="arrange_output", dev="tikz", fig.height=4, fig.width=5, fig.cap="Uncertainty in the model output. a) Uncertainty in the model goodness of fit. All sources of uncertainty have been taken into account except the selection between OLS and OLS robust (trigger X2). Robust OLS does not allow to compute $r^2$. b) Uncertainty in the slope."}

# PLOT UNCERTAINTY ------------------------------------------------------------

# Plot r2
a <- ggplot(AB.dt, aes(r.squared)) + 
  geom_histogram(color = "black", fill = "white") + 
  theme_AP() +
  labs(x = expression(r ^ 2), 
       y = "Density") +
  scale_y_continuous(breaks = pretty_breaks(n = 3)) +
  scale_x_continuous(breaks = pretty_breaks(n = 3)) +
  facet_wrap(~Continent, ncol = 4) +
  theme(panel.spacing.x = unit(4, "mm"))

# Plot beta
b <- ggplot(AB.dt, aes(Beta)) + 
  geom_histogram(color = "black", fill = "white") + 
  theme_AP() +
  geom_vline(xintercept = 1, 
             lty = 2, 
             color = "red") +
  labs(x = "$\\beta$", 
       y = "") +
  scale_y_continuous(breaks = pretty_breaks(n = 3)) +
  scale_x_continuous(breaks = pretty_breaks(n = 3)) +
  facet_wrap(~Continent, ncol = 4) +
  theme(panel.spacing.x = unit(4, "mm"))


# Merge
plot_grid(a, b, ncol = 1, align = "hv", labels = "auto")
```

```{r check_order_r2, cache=TRUE, dependson="arrange_output", fig.height=7, fig.width=4.5}

# CHECK THE COMBINATIONS LEADING TO HIGHEST R2 VALUES (first 200) -------------

# Check min and max of first 200 r.squared values
AB.dt[order(-r.squared), head(.SD, 200), Continent][
  , .(min = min(r.squared), max = max(r.squared)), Continent]

# Create plot
dd <- AB.dt[order(-r.squared), head(.SD, 200), Continent] %>%
  .[, ID:= paste(X1, X2, X4, X5, sep = "_")] %>%
  .[, .N, .(Continent, ID)]

tmp <- split(dd, dd$Continent)
gg <- list()
for(i in names(tmp)) {
  gg[[i]] <- ggplot(tmp[[i]], aes(reorder(ID, N), N)) +
    geom_bar(stat = "identity", 
             color = "black", 
             fill = "white") + 
    coord_flip() +
    labs(y = expression(italic(N)), 
         x = "") +
    theme_AP() +
    ggtitle(names(tmp[i]))
}

gg
```

```{r super_sub, cache=TRUE, dependson="arrange_output"}

# SUPERLINEAR OR SUBLINEAR REGIME? --------------------------------------------

AB.dt[, .(sublinear = sum(Beta < 1, na.rm = TRUE) / .N, 
          superlinear = sum(Beta > 1, na.rm = TRUE) / .N), 
      Continent]

AB.dt[, .(">90" = sum(r.squared > 0.9) / .N,
            "75-90" = sum(r.squared > 0.75 & r.squared < 0.9) / .N, 
            "50-75" = sum(r.squared > 0.50 & r.squared < 0.75) / .N, 
            "25-50" = sum(r.squared > 0.25 & r.squared < 0.5) / .N), Continent]
```

# Sensitivity analysis

Here we conduct the sensitivity analysis to appraise how much uncertainty in $\beta$ is conveyed by each of the triggers. We use the @Saltelli2010a and the @Jansen1999 estimators to compute first-order effects ($S_i$) and total-order effects ($S_{Ti}$) respectively, as per the established best practices.Figure 8 plots the results. It seems that the trigger conveying the most uncertainty in the value of $\beta$ is X1, which reflects the uncertainty in the current extension of irrigation, followed by X6, which represents the uncertainty in the true value of $\beta$. X3 (i.e. the trigger that decides whether to correct the regressions for outliers/heteroskedasticity) does not have any effect. It is worth mentioning the presence of significant interactions between the triggers: below we show that they are responsible for 10–25\% of the uncertainty in $\beta$. Figures 9 and 10 indicate the presence of relevant second and third-order interactions in the case of the Americas and Europe.

```{r scatterplots, cache=TRUE, dependson="arrange_output", fig.height=5, fig.width=7}

# PLOT SCATTERPLOTS OF PARAMETERS VS MODEL OUTPUT -----------------------------

AB.dt <- AB.dt[, X5:= factor(X5, levels = as.factor(1:m.iterations))]

scatter.dt <- melt(AB.dt, measure.vars = paste("X", 1:5, sep = "")) %>%
  .[, Regime:= ifelse(Beta > 1, "Super-linear", "Sub-linear")]

# Beta
ggplot(scatter.dt, aes(value, Beta, color = Regime)) +
  geom_point(alpha = 0.3, size = 0.5) +
  facet_grid(Continent ~ variable,
             scales = "free_x") +
  geom_hline(yintercept = 1,
             lty = 2) +
  scale_color_manual(values = c("#00BFC4", "#F8766D")) +
  theme_AP() +
  labs(x = "", y = expression(beta)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "top")

# R squared
ggplot(scatter.dt, aes(value, r.squared, color = Regime)) +
  geom_point(alpha = 0.3, size = 0.5) +
  facet_grid(Continent ~ variable,
             scales = "free_x") +
  labs(x = "", 
       y = expression(italic(r)^2)) +
  scale_color_manual(values = c("#00BFC4", "#F8766D")) +
  theme_AP() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "top")
```

```{r sensitivity_settings, cache=TRUE}

# SENSITIVITY SETTINGS --------------------------------------------------------

# Number of bootstrap replicas
R <- 1000
```

```{r sensitivity, cache=TRUE, dependson=c("arrange_output", "set_sample_matrix")}

# SENSITIVITY ANALYSIS --------------------------------------------------------

# Beta
indices <- sample.matrix.dt[, sobol_indices(Y = Beta, 
                                            N = n,
                                            params = parameters, 
                                            first = "jansen",
                                            R = R, 
                                            boot = TRUE,
                                            parallel = "multicore", 
                                            ncpus = n_cores, 
                                            order = "third"), 
                            Continent]

# r squared
indicesR <- sample.matrix.dt[, sobol_indices(Y = r.squared, 
                                             N = n,
                                             params = parameters, 
                                             first = "jansen",
                                             R = R, 
                                             boot = TRUE,
                                             parallel = "multicore", 
                                             ncpus = n_cores, 
                                             order = "third"), 
                             Continent]

# Compute Sobol' indices for the dummy parameter (Beta)
indices.dummy <- sample.matrix.dt[, sobol_dummy(Y = Beta, 
                                                N = n,
                                                params = parameters, 
                                                R = R, 
                                                boot = TRUE,
                                                parallel = "multicore", 
                                                ncpus = n_cores), 
                                  Continent]

# Compute Sobol' indices for the dummy parameter (r squared)
indices.dummyR <- sample.matrix.dt[, sobol_dummy(Y = r.squared, 
                                                 N = n,
                                                 params = parameters, 
                                                 R = R, 
                                                 boot = TRUE,
                                                 parallel = "multicore", 
                                                 ncpus = n_cores), 
                                   Continent]
```


```{r plot_sobol, cache=TRUE, dependson="ci", dev="tikz", fig.cap="Sobol' indices.", fig.width = 5}

# PLOT SOBOL' INDICES ---------------------------------------------------------

a <- ggplot(indices[sensitivity %in% c("Si", "Ti")], aes(parameters, original, fill = sensitivity)) +
  geom_bar(stat = "identity",
           position = position_dodge(0.6),
           color = "black") +
  geom_errorbar(aes(ymin = low.ci,
                    ymax = high.ci),
                position = position_dodge(0.6)) +
  scale_y_continuous(breaks = pretty_breaks(n = 3)) +
  facet_wrap(~Continent,
             ncol = 4) +
  labs(x = "",
       y = "Sobol' index") +
  scale_fill_discrete(name = "Sobol' indices",
                      labels = c(expression(S[italic(i)]),
                                 expression(T[italic(i)]))) +
  theme_AP() +
  theme(legend.position = "none")


b <- ggplot(indicesR[sensitivity %in% c("Si", "Ti")], aes(parameters, original, fill = sensitivity)) +
  geom_bar(stat = "identity",
           position = position_dodge(0.6),
           color = "black") +
  geom_errorbar(aes(ymin = low.ci,
                    ymax = high.ci),
                position = position_dodge(0.6)) +
  scale_y_continuous(breaks = pretty_breaks(n = 3)) +
  facet_wrap(~Continent,
             ncol = 4) +
  labs(x = "",
       y = "Sobol' index") +
  scale_fill_discrete(name = "Sobol' indices",
                      labels = c(expression(S[italic(i)]),
                                 expression(T[italic(i)]))) +
  theme_AP() +
  theme(legend.position = "none")

legend <- get_legend(a + theme(legend.position = "top"))

all <- plot_grid(a, b, ncol = 1, align = "hv", labels = "auto")

plot_grid(legend, all, ncol = 1,  rel_heights = c(0.1, 1))
```

```{r sum_si, cache=TRUE, dependson="ci"}

# CHECK SUM OF FIRST-ORDER INDICES --------------------------------------------

lapply(list(indices, indicesR), function(x) 
  x[sensitivity == "Si"] %>%
      .[, sum(original), Continent])
```

```{r plot_sobol_second_third, cache=TRUE, dependson="ci", dev = "tikz", fig.cap="High-order interactions between the triggers."}

# PLOT SOBOL' INDICES (SECOND AND THIRD ORDER) --------------------------------

lapply(c("Sij", "Sijk"), function(x)
  ggplot(indices[sensitivity == x], aes(reorder(parameters, original), original)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = low.ci, 
                    ymax = high.ci)) + 
  facet_wrap(~Continent) +
  theme_bw() + 
  labs(x = "", y = "Sobol' index") + 
  geom_hline(yintercept = 0, lty = 2, color = "red") + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        legend.background = element_rect(fill = "transparent", 
                                         color = NA), 
        legend.key = element_rect(fill = "transparent", 
                                  color = NA), 
        axis.text.x = element_text(angle = 45, 
                                   hjust = 1)))
```

\clearpage

# Session information

```{r session_information}

# SESSION INFORMATION ---------------------------------------------------------

sessionInfo()

## Return the machine CPU
cat("Machine: "); print(get_cpu()$model_name)

## Return number of true cores
cat("Num cores: "); print(detectCores(logical = FALSE))

## Return number of threads
cat("Num threads: "); print(detectCores(logical = TRUE))

## Return the machine RAM
cat("RAM: "); print (get_ram()); cat("\n")

```

# References